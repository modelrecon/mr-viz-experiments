{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformer Visualization Demo\n",
                "\n",
                "This notebook demonstrates how to use the this library to visualize the internal activations of a quantized GPT-2 model, just create a `intrav_viz` directory on colab and upload the python files init, hook_manager and visualizer.\n",
                "Requirements: Make sure the `intra_viz` folder is uploaded to the same directory as this notebook."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers accelerate bitsandbytes plotly"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "# Import directly from the uploaded package\n",
                "from intra_viz import TransformerVisualizer\n",
                "\n",
                "# Check for GPU\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Quantized Model (4-bit)\n",
                "model_id = \"gpt2\"\n",
                "\n",
                "if device == \"cuda\":\n",
                "    quantization_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_compute_dtype=torch.float16\n",
                "    )\n",
                "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
                "else:\n",
                "    # Fallback for CPU (no bitsandbytes 4bit support on CPU usually)\n",
                "    print(\"Quantization not supported on CPU, loading standard model.\")\n",
                "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "tokenizer.pad_token = tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Visualizer\n",
                "visualizer = TransformerVisualizer(model, tokenizer, device=device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate Visualization\n",
                "input_text = \"The quick brown fox jumps over the lazy dog\"\n",
                "fig = visualizer.visualize(input_text, top_k=10)\n",
                "\n",
                "# Display\n",
                "fig.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
